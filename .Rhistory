pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((Hitters$Salary[test] - pred)^2)
}
val.errors
which.min(val.errors)
plot(val.errors, type="b")
coef(regfit.best, 7)
predict.regsubsets <- function(object, newdata, id, ...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id = id)
xvars <- names(coefi)
mat[, xvars] %*% coefi
}
regfit.best <- regsubsets(Salary ~ ., data = Hitters,
nvmax = 19)
coef(regfit.best, 7)
k <- 10
n <- nrow(Hitters)
set.seed(1)
folds <- sample(rep(1:k, length = n))
cv.errors <- matrix(NA, k, 19,
dimnames = list(NULL, paste(1:19)))
for (j in 1:k) {
best.fit <- regsubsets(Salary ~ .,
data = Hitters[folds != j, ],
nvmax = 19)
for (i in 1:19) {
pred <- predict(best.fit, Hitters[folds == j, ], id = i)
cv.errors[j, i] <-
mean((Hitters$Salary[folds == j] - pred)^2)
}
}
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
# par(mfrow = c(1, 1))
plot(mean.cv.errors, type = "b")
reg.best <- regsubsets(Salary ~ ., data = Hitters,
nvmax = 19)
coef(reg.best, 10)
x <- model.matrix(Salary ~ ., Hitters)[, -1]
y <- Hitters$Salary
library(glmnet)
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[, 50]
sqrt(sum(coef(ridge.mod)[-1, 50]^2))
ridge.mod$lambda[60]
coef(ridge.mod)[, 60]
sqrt(sum(coef(ridge.mod)[-1, 60]^2))
predict(ridge.mod, s = 50, type = "coefficients")[1:20, ]
set.seed(1)
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]
ridge.mod <- glmnet(x[train, ], y[train], alpha = 0,
lambda = grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s = 4, newx = x[test, ])
mean((ridge.pred - y.test)^2)
mean((mean(y[train]) - y.test)^2)
ridge.pred <- predict(ridge.mod, s = 1e10, newx = x[test, ])
mean((ridge.pred - y.test)^2)
ridge.pred <- predict(ridge.mod, s = 0, newx = x[test, ],
exact = T, x = x[train, ], y = y[train])
mean((ridge.pred - y.test)^2)
lm(y ~ x, subset = train)
predict(ridge.mod, s = 0, exact = T, type = "coefficients",
x = x[train, ], y = y[train])[1:20, ]
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv.out)
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test, ])
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test, ])
library(ISLR2)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters)) # there are no more missing values
library(leaps)
regfit.full <- regsubsets(Salary ~ ., Hitters) # very fast by default find best models with 1,2,...,8 predictors
summary(regfit.full)
regfit.full <- regsubsets(Salary ~ ., data = Hitters,
nvmax = 19) # up to 19 predictors (all of them)
reg.summary <- summary(regfit.full)
names(reg.summary)
reg.summary$rsq
# par(mfrow = c(1, 2))
plot(reg.summary$rss, xlab = "Number of Variables",
ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables",
ylab = "Adjusted RSq", type = "l")
which.max(reg.summary$adjr2)
plot(reg.summary$adjr2, xlab = "Number of Variables",
ylab = "Adjusted RSq", type = "l")
points(11, reg.summary$adjr2[11], col = "red", cex = 2,
pch = 20)
plot(reg.summary$cp, xlab = "Number of Variables",
ylab = "Cp", type = "l")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col = "red", cex = 2,
pch = 20)
which.min(reg.summary$bic)
plot(reg.summary$bic, xlab = "Number of Variables",
ylab = "BIC", type = "l")
points(6, reg.summary$bic[6], col = "red", cex = 2,
pch = 20)
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")
coef(regfit.full, 6)
regfit.fwd <- regsubsets(Salary ~ ., data = Hitters,
nvmax = 19, method = "forward") # here best models are nested
summary(regfit.fwd)
regfit.bwd <- regsubsets(Salary ~ ., data = Hitters,
nvmax = 19, method = "backward") # here best models are nested
summary(regfit.bwd)
coef(regfit.full, 7)
coef(regfit.fwd, 7)
coef(regfit.bwd, 7)
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(Hitters),
replace = TRUE)
test <- (!train)
regfit.best <- regsubsets(Salary ~ .,
data = Hitters[train, ], nvmax = 19)
test.mat <- model.matrix(Salary ~ ., data = Hitters[test, ])
val.errors <- rep(NA, 19)
for (i in 1:19) {
coefi <- coef(regfit.best, id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((Hitters$Salary[test] - pred)^2)
}
val.errors
which.min(val.errors)
plot(val.errors, type="b")
coef(regfit.best, 7)
predict.regsubsets <- function(object, newdata, id, ...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id = id)
xvars <- names(coefi)
mat[, xvars] %*% coefi
}
regfit.best <- regsubsets(Salary ~ ., data = Hitters,
nvmax = 19)
coef(regfit.best, 7)
k <- 10
n <- nrow(Hitters)
set.seed(1)
folds <- sample(rep(1:k, length = n))
cv.errors <- matrix(NA, k, 19,
dimnames = list(NULL, paste(1:19)))
for (j in 1:k) {
best.fit <- regsubsets(Salary ~ .,
data = Hitters[folds != j, ],
nvmax = 19)
for (i in 1:19) {
pred <- predict(best.fit, Hitters[folds == j, ], id = i)
cv.errors[j, i] <-
mean((Hitters$Salary[folds == j] - pred)^2)
}
}
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
# par(mfrow = c(1, 1))
plot(mean.cv.errors, type = "b")
reg.best <- regsubsets(Salary ~ ., data = Hitters,
nvmax = 19)
coef(reg.best, 10)
x <- model.matrix(Salary ~ ., Hitters)[, -1]
y <- Hitters$Salary
library(glmnet)
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[, 50]
sqrt(sum(coef(ridge.mod)[-1, 50]^2))
ridge.mod$lambda[60]
coef(ridge.mod)[, 60]
sqrt(sum(coef(ridge.mod)[-1, 60]^2))
predict(ridge.mod, s = 50, type = "coefficients")[1:20, ]
set.seed(1)
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]
ridge.mod <- glmnet(x[train, ], y[train], alpha = 0,
lambda = grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s = 4, newx = x[test, ])
mean((ridge.pred - y.test)^2)
mean((mean(y[train]) - y.test)^2)
ridge.pred <- predict(ridge.mod, s = 1e10, newx = x[test, ])
mean((ridge.pred - y.test)^2)
ridge.pred <- predict(ridge.mod, s = 0, newx = x[test, ],
exact = T, x = x[train, ], y = y[train])
mean((ridge.pred - y.test)^2)
lm(y ~ x, subset = train)
predict(ridge.mod, s = 0, exact = T, type = "coefficients",
x = x[train, ], y = y[train])[1:20, ]
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min # save best value of lambda
bestlam
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test, ])
mean((ridge.pred - y.test)^2)
bestlam
mean((ridge.pred - y.test)^2)
out <- glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)[1:20, ]
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
plot(lasso.mod, xvar="lambda")
set.seed(1)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test, ])
mean((lasso.pred - y.test)^2)
out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam)[1:20, ]
lasso.coef
lasso.coef[lasso.coef != 0]
library(pls)
set.seed(2)
summary(pcr.fit)
summary(pcr.fit)
library(pls)
set.seed(2)
pcr.fit <- pcr(Salary ~ ., data = Hitters, scale = TRUE, validation = "CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type = "MSEP")
set.seed(1)
pcr.fit <- pcr(Salary ~ ., data = Hitters, subset = train,
scale = TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
pcr.pred <- predict(pcr.fit, x[test, ], ncomp = 5)
mean((pcr.pred - y.test)^2)
pcr.fit <- pcr(y ~ x, scale = TRUE, ncomp = 5)
summary(pcr.fit)
set.seed(1)
pls.fit <- plsr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
pls.pred <- predict(pls.fit, x[test, ], ncomp = 1)
mean((pls.pred - y.test)^2)
pls.fit <- plsr(Salary ~ ., data = Hitters, scale = TRUE, ncomp = 1)
summary(pls.fit)
library(ISLR2)
attach(Wage)
fit <- lm(wage ~ poly(age, 4), data = Wage)
summary(fit)
fit2 <- lm(wage ~ poly(age, 4, raw = T), data = Wage)
summary(fit2)
summary(fit2)
fit2a <- lm(wage ~ age + I(age^2) + I(age^3) + I(age^4),
data = Wage)
coef(fit2a)
fit2b <- lm(wage ~ cbind(age, age^2, age^3, age^4),
data = Wage)
agelims <- range(age)
agelims
age.grid <- seq(from = agelims[1], to = agelims[2])
age.grid
preds <- predict(fit, newdata = list(age = age.grid),
se = TRUE)
se.bands <- cbind(preds$fit + 2 * preds$se.fit,
preds$fit - 2 * preds$se.fit)
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Degree-4 Polynomial", outer = T)
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Degree-4 Polynomial", outer = T)
lines(age.grid, preds$fit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lwd = 1, col = "blue", lty = 3)
preds2 <- predict(fit2, newdata = list(age = age.grid),
se = TRUE)
max(abs(preds$fit - preds2$fit))
plot(preds$fit, preds2$fit)
fit.1 <- lm(wage ~ age, data = Wage)
fit.2 <- lm(wage ~ poly(age, 2), data = Wage)
fit.3 <- lm(wage ~ poly(age, 3), data = Wage)
fit.4 <- lm(wage ~ poly(age, 4), data = Wage)
fit.5 <- lm(wage ~ poly(age, 5), data = Wage)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
summary(fit.5)
(-11.983)^2
fit.1 <- lm(wage ~ education + age, data = Wage)
fit.2 <- lm(wage ~ education + poly(age, 2), data = Wage)
fit.3 <- lm(wage ~ education + poly(age, 3), data = Wage)
anova(fit.1, fit.2, fit.3)
fit <- glm(I(wage > 250) ~ poly(age, 4), data = Wage,
family = binomial)
preds <- predict(fit, newdata = list(age = age.grid), se = T)
pfit <- exp(preds$fit) / (1 + exp(preds$fit))
se.bands.logit <- cbind(preds$fit + 2 * preds$se.fit,
preds$fit - 2 * preds$se.fit)
se.bands <- exp(se.bands.logit) / (1 + exp(se.bands.logit))
preds <- predict(fit, newdata = list(age = age.grid),
type = "response", se = T)
plot(age, I(wage > 250), xlim = agelims, type = "n",
ylim = c(0, .2))
points(jitter(age), I((wage > 250) / 5), cex = .5, pch = "|", col = "darkgrey")
lines(age.grid, pfit, lwd = 2, col = "blue")
matlines(age.grid, se.bands, lwd = 1, col = "blue", lty = 3)
table(cut(age, 4))
table(cut(age, 4))
fit <- lm(wage ~ cut(age, 4), data = Wage)
coef(summary(fit))
library(splines)
fit <- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage)
pred <- predict(fit, newdata = list(age = age.grid), se = T)
library(splines)
fit <- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage)
pred <- predict(fit, newdata = list(age = age.grid), se = T)
plot(age, wage, col = "gray")
lines(age.grid, pred$fit, lwd = 2)
lines(age.grid, pred$fit + 2 * pred$se, lty = "dashed")
lines(age.grid, pred$fit - 2 * pred$se, lty = "dashed")
dim(bs(age, knots = c(25, 40, 60)))
dim(bs(age, df = 6))
attr(bs(age, df = 6), "knots")
fit2 <- lm(wage ~ ns(age, df = 4), data = Wage)
pred2 <- predict(fit2, newdata = list(age = age.grid),
se = T)
plot(age, wage, col = "gray")
lines(age.grid, pred2$fit, col = "red", lwd = 2)
fit2 <- lm(wage ~ ns(age, df = 4), data = Wage)
pred2 <- predict(fit2, newdata = list(age = age.grid),
se = T)
plot(age, wage, col = "gray")
lines(age.grid, pred2$fit, col = "red", lwd = 2)
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Smoothing Spline")
fit <- smooth.spline(age, wage, df = 16)
fit2 <- smooth.spline(age, wage, cv = T)
fit2$df
lines(fit, col = "red", lwd = 2)
lines(fit2, col = "blue", lwd = 2)
legend("topright", legend = c("16 DF", "6.8 DF"),
col = c("red", "blue"), lty = 1, lwd = 2, cex = .8)
plot(age, wage, xlim = agelims, cex = .5, col = "darkgrey")
title("Local Regression")
fit <- loess(wage ~ age, span = .2, data = Wage)
fit2 <- loess(wage ~ age, span = .5, data = Wage)
lines(age.grid, predict(fit, data.frame(age = age.grid)),
col = "red", lwd = 2)
lines(age.grid, predict(fit2, data.frame(age = age.grid)),
col = "blue", lwd = 2)
legend("topright", legend = c("Span = 0.2", "Span = 0.5"),
col = c("red", "blue"), lty = 1, lwd = 2, cex = .8)
gam1 <- lm(wage ~ ns(year, 4) + ns(age, 5) + education,
data = Wage)
library(gam)
gam.m3 <- gam(wage ~ s(year, 4) + s(age, 5) + education,
data = Wage)
library(gam)
gam.m3 <- gam(wage ~ s(year, 4) + s(age, 5) + education,
data = Wage)
par(mfrow = c(1, 3))
plot(gam.m3, se = TRUE, col = "blue")
plot.Gam(gam1, se = TRUE, col = "red")
gam.m1 <- gam(wage ~ s(age, 5) + education, data = Wage)
gam.m2 <- gam(wage ~ year + s(age, 5) + education,
data = Wage)
anova(gam.m1, gam.m2, gam.m3, test = "F")
summary(gam.m3)
preds <- predict(gam.m2, newdata = Wage)
gam.lo <- gam(
wage ~ s(year, df = 4) + lo(age, span = 0.7) + education,
data = Wage
)
plot.Gam(gam.lo, se = TRUE, col = "green")
9+13+45+30
7+48+63+8
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(MASS)
library(ISLR2)
"Tibet" <-
structure(.Data = list(c(190.5, 172.5, 167., 169.5, 175., 177.5, 179.5, 179.5, 173.5, 162.5, 178.5, 171.5, 180.5, 183., 169.5, 172., 170.,
182.5, 179.5, 191., 184.5, 181., 173.5, 188.5, 175., 196., 200., 185., 174.5, 195.5, 197., 182.5)
, c(152.5, 132., 130., 150.5, 138.5, 142.5, 142.5, 138., 135.5, 139., 135., 148.5, 139., 149., 130., 140., 126.5,
136., 135., 140.5, 141.5, 142., 136.5, 130., 153., 142.5, 139.5, 134.5, 143.5, 144., 131.5, 131.)
, c(145., 125.5, 125.5, 133.5, 126., 142.5, 127.5, 133.5, 130.5, 131., 136., 132.5, 132., 121.5, 131., 136., 134.5,
138.5, 128.5, 140.5, 134.5, 132.5, 126., 143., 130., 123.5, 143.5, 140., 132.5, 138.5, 135., 135.)
, c(73.5, 63., 69.5, 64.5, 77.5, 71.5, 70.5, 73.5, 70., 62., 71., 65., 74.5, 76.5, 68., 70.5, 66., 76., 74., 72.5,
76.5, 79., 71.5, 79.5, 76.5, 76., 82.5, 81.5, 74., 78.5, 80.5, 68.5)
, c(136.5, 121., 119.5, 128., 135.5, 131., 134.5, 132.5, 133.5, 126., 124., 146.5, 134.5, 142., 119., 133.5, 118.5,
134., 132., 131.5, 141.5, 136.5, 136.5, 136., 142., 134., 146., 137., 136.5, 144., 139., 136.)
, structure(.Data = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)
, levels = c("1", "2")
, class = "factor"
)
)
, names = c("Length", "Breadth", "Height", "Fheight", "Fbreadth", "Type")
, row.names = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20",
"21", "22", "23", "24", "25", "26", "27", "28", "29", "30", "31", "32")
, class = "data.frame"
)
Tibet[1:4,]
Tibet.col<-ifelse(Tibet$Type==2,"orange","blue")
pairs(Tibet[,1:5],col=Tibet.col)
lda.fit<-lda(Type~.,data=Tibet)
lda.fit
fit<-glm(Type ~ ., data=Tibet, family=binomial)
summary(fit)
Auto[1:4,]
set.seed(18)
train <- sample(1:nrow(Auto), nrow(Auto) / 2)
test <- (-train)
library(glmnet)
y <- Auto$mpg
x <- model.matrix(mpg~ .,data = Auto)
set.seed(18)
cv.out <- cv.glmnet(x[train, ], y[train], alpha =1)
plot(cv.out)
print(Auto[1:4,], size=0.5)
print(Auto[1:4,], size=0.5)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(MASS)
library(ISLR2)
"Tibet" <-
structure(.Data = list(c(190.5, 172.5, 167., 169.5, 175., 177.5, 179.5, 179.5, 173.5, 162.5, 178.5, 171.5, 180.5, 183., 169.5, 172., 170.,
182.5, 179.5, 191., 184.5, 181., 173.5, 188.5, 175., 196., 200., 185., 174.5, 195.5, 197., 182.5)
, c(152.5, 132., 130., 150.5, 138.5, 142.5, 142.5, 138., 135.5, 139., 135., 148.5, 139., 149., 130., 140., 126.5,
136., 135., 140.5, 141.5, 142., 136.5, 130., 153., 142.5, 139.5, 134.5, 143.5, 144., 131.5, 131.)
, c(145., 125.5, 125.5, 133.5, 126., 142.5, 127.5, 133.5, 130.5, 131., 136., 132.5, 132., 121.5, 131., 136., 134.5,
138.5, 128.5, 140.5, 134.5, 132.5, 126., 143., 130., 123.5, 143.5, 140., 132.5, 138.5, 135., 135.)
, c(73.5, 63., 69.5, 64.5, 77.5, 71.5, 70.5, 73.5, 70., 62., 71., 65., 74.5, 76.5, 68., 70.5, 66., 76., 74., 72.5,
76.5, 79., 71.5, 79.5, 76.5, 76., 82.5, 81.5, 74., 78.5, 80.5, 68.5)
, c(136.5, 121., 119.5, 128., 135.5, 131., 134.5, 132.5, 133.5, 126., 124., 146.5, 134.5, 142., 119., 133.5, 118.5,
134., 132., 131.5, 141.5, 136.5, 136.5, 136., 142., 134., 146., 137., 136.5, 144., 139., 136.)
, structure(.Data = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)
, levels = c("1", "2")
, class = "factor"
)
)
, names = c("Length", "Breadth", "Height", "Fheight", "Fbreadth", "Type")
, row.names = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20",
"21", "22", "23", "24", "25", "26", "27", "28", "29", "30", "31", "32")
, class = "data.frame"
)
Tibet[1:4,]
Tibet.col<-ifelse(Tibet$Type==2,"orange","blue")
pairs(Tibet[,1:5],col=Tibet.col)
lda.fit<-lda(Type~.,data=Tibet)
lda.fit
fit<-glm(Type ~ ., data=Tibet, family=binomial)
summary(fit)
print(Auto[1:4,], size=0.5)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(MASS)
library(ISLR2)
"Tibet" <-
structure(.Data = list(c(190.5, 172.5, 167., 169.5, 175., 177.5, 179.5, 179.5, 173.5, 162.5, 178.5, 171.5, 180.5, 183., 169.5, 172., 170.,
182.5, 179.5, 191., 184.5, 181., 173.5, 188.5, 175., 196., 200., 185., 174.5, 195.5, 197., 182.5)
, c(152.5, 132., 130., 150.5, 138.5, 142.5, 142.5, 138., 135.5, 139., 135., 148.5, 139., 149., 130., 140., 126.5,
136., 135., 140.5, 141.5, 142., 136.5, 130., 153., 142.5, 139.5, 134.5, 143.5, 144., 131.5, 131.)
, c(145., 125.5, 125.5, 133.5, 126., 142.5, 127.5, 133.5, 130.5, 131., 136., 132.5, 132., 121.5, 131., 136., 134.5,
138.5, 128.5, 140.5, 134.5, 132.5, 126., 143., 130., 123.5, 143.5, 140., 132.5, 138.5, 135., 135.)
, c(73.5, 63., 69.5, 64.5, 77.5, 71.5, 70.5, 73.5, 70., 62., 71., 65., 74.5, 76.5, 68., 70.5, 66., 76., 74., 72.5,
76.5, 79., 71.5, 79.5, 76.5, 76., 82.5, 81.5, 74., 78.5, 80.5, 68.5)
, c(136.5, 121., 119.5, 128., 135.5, 131., 134.5, 132.5, 133.5, 126., 124., 146.5, 134.5, 142., 119., 133.5, 118.5,
134., 132., 131.5, 141.5, 136.5, 136.5, 136., 142., 134., 146., 137., 136.5, 144., 139., 136.)
, structure(.Data = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)
, levels = c("1", "2")
, class = "factor"
)
)
, names = c("Length", "Breadth", "Height", "Fheight", "Fbreadth", "Type")
, row.names = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20",
"21", "22", "23", "24", "25", "26", "27", "28", "29", "30", "31", "32")
, class = "data.frame"
)
Tibet[1:4,]
Tibet.col<-ifelse(Tibet$Type==2,"orange","blue")
pairs(Tibet[,1:5],col=Tibet.col)
lda.fit<-lda(Type~.,data=Tibet)
lda.fit
fit<-glm(Type ~ ., data=Tibet, family=binomial)
summary(fit)
Auto[1:4,]
set.seed(18)
train <- sample(1:nrow(Auto), nrow(Auto) / 2)
test <- (-train)
library(glmnet)
y <- Auto$mpg
x <- model.matrix(mpg~ .,data = Auto)
set.seed(18)
cv.out <- cv.glmnet(x[train, ], y[train], alpha =1)
plot(cv.out)
(18+24)/2
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
options(width = 999)
library(MASS)
library(ISLR2)
is.na(Auto)
sum(is.na(Auto))
mod1=lm(mpg~., data=Auto)
mod1
summary(mod1)
mod1=lm(mpg~cylinders+displacement+horsepower+weight+acceleration+year, data=Auto)
summary(mod1)
mod1=lm(mpg~cylinders+displacement+horsepower+weight+acceleration+year, data=Auto)
mod2=lm(mpg~weight+year, data=Auto)
summary(mod1)
summary(mod2)
source("~/Dropbox/GitHub/Dynamic-Probit-EP/Functions.R", echo=TRUE)
source("~/Dropbox/GitHub/Dynamic-Probit-EP/DynamicEP.R", echo=TRUE)
